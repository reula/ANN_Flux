{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85132db0-ccac-4c2a-94c0-a5031dfa3e42",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Unsatisfiable requirements detected for package \u001b[38;5;5mJLD2 [033835bb]\u001b[39m:\n \u001b[38;5;5mJLD2 [033835bb]\u001b[39m log:\n ├─\u001b[38;5;5mJLD2 [033835bb]\u001b[39m has no known versions!\n └─restricted to versions \u001b[38;5;5m*\u001b[39m by an explicit requirement — no versions left",
     "output_type": "error",
     "traceback": [
      "Unsatisfiable requirements detected for package \u001b[38;5;5mJLD2 [033835bb]\u001b[39m:\n \u001b[38;5;5mJLD2 [033835bb]\u001b[39m log:\n ├─\u001b[38;5;5mJLD2 [033835bb]\u001b[39m has no known versions!\n └─restricted to versions \u001b[38;5;5m*\u001b[39m by an explicit requirement — no versions left",
      "",
      "Stacktrace:",
      "  [1] check_constraints(graph::Pkg.Resolve.Graph)",
      "    @ Pkg.Resolve /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Resolve/graphtype.jl:978",
      "  [2] Pkg.Resolve.Graph(versions::Dict{Base.UUID, Set{VersionNumber}}, deps::Dict{Base.UUID, Dict{VersionNumber, Dict{String, Base.UUID}}}, compat::Dict{Base.UUID, Dict{VersionNumber, Dict{String, Pkg.Types.VersionSpec}}}, uuid_to_name::Dict{Base.UUID, String}, reqs::Dict{Base.UUID, Pkg.Types.VersionSpec}, fixed::Dict{Base.UUID, Pkg.Resolve.Fixed}, verbose::Bool, julia_version::VersionNumber)",
      "    @ Pkg.Resolve /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Resolve/graphtype.jl:371",
      "  [3] deps_graph(ctx::Pkg.Types.Context, uuid_to_name::Dict{Base.UUID, String}, reqs::Dict{Base.UUID, Pkg.Types.VersionSpec}, fixed::Dict{Base.UUID, Pkg.Resolve.Fixed})",
      "    @ Pkg.Operations /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:534",
      "  [4] resolve_versions!(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec})",
      "    @ Pkg.Operations /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:404",
      "  [5] targeted_resolve(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec}, preserve::Pkg.Types.PreserveLevel)",
      "    @ Pkg.Operations /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:1210",
      "  [6] tiered_resolve(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec})",
      "    @ Pkg.Operations /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:1196",
      "  [7] _resolve",
      "    @ /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:1216 [inlined]",
      "  [8] add(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec}, new_git::Vector{Base.UUID}; preserve::Pkg.Types.PreserveLevel, platform::Base.BinaryPlatforms.Platform)",
      "    @ Pkg.Operations /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/Operations.jl:1231",
      "  [9] add(ctx::Pkg.Types.Context, pkgs::Vector{Pkg.Types.PackageSpec}; preserve::Pkg.Types.PreserveLevel, platform::Base.BinaryPlatforms.Platform, kwargs::Base.Iterators.Pairs{Symbol, IOContext{Base.PipeEndpoint}, Tuple{Symbol}, NamedTuple{(:io,), Tuple{IOContext{Base.PipeEndpoint}}}})",
      "    @ Pkg.API /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:203",
      " [10] add(pkgs::Vector{Pkg.Types.PackageSpec}; io::IOContext{Base.PipeEndpoint}, kwargs::Base.Iterators.Pairs{Union{}, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ Pkg.API /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:79",
      " [11] add(pkgs::Vector{Pkg.Types.PackageSpec})",
      "    @ Pkg.API /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:77",
      " [12] #add#23",
      "    @ /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:75 [inlined]",
      " [13] add",
      "    @ /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:75 [inlined]",
      " [14] #add#22",
      "    @ /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:74 [inlined]",
      " [15] add(pkg::String)",
      "    @ Pkg.API /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Pkg/src/API.jl:74",
      " [16] top-level scope",
      "    @ In[1]:1",
      " [17] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [18] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"Flux\")\n",
    "import Pkg; Pkg.add(\"Surrogates\")\n",
    "import Pkg; Pkg.add(\"ProgressMeter\")\n",
    "using Flux\n",
    "using Plots\n",
    "using LinearAlgebra\n",
    "using ProgressMeter\n",
    "using Statistics\n",
    "using LaTeXStrings\n",
    "using Surrogates\n",
    "gr()\n",
    "\n",
    "# Define function that we would like to learn with our neural network\n",
    "f(x) = x[1].^2 + x[2].^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664a1ed-236e-432c-a877-f7b9ac535f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "lower_bound = [-1.0, -1.0]\n",
    "upper_bound = [1.0, 1.0]\n",
    "\n",
    "xys = Surrogates.sample(n_samples, lower_bound, upper_bound, SobolSample())\n",
    "rawInputs = xys\n",
    "rawOutputs = [[f(xy)] for xy in xys] # Compute outputs for each input\n",
    "trainingData = zip(rawInputs, rawOutputs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1e96f3-12d4-4d6e-bc9a-1ff4660d5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network layers (this defines a function called model(x))\n",
    "# Specify our model\n",
    "dim_input = 2\n",
    "dim_ouptut = 1\n",
    "Q1 = 784;\n",
    "Q2 = 50;\n",
    "\n",
    "# Two inputs, one output\n",
    "model = Chain(Dense(2,Q1,relu),\n",
    "            Dense(Q1,Q2,relu),\n",
    "            Dense(Q2,1,identity));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ed4b8-3380-4e3e-a598-448d2b5703cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and weights\n",
    "loss(x, y) = Flux.Losses.mse(model(collect(x)), y)\n",
    "\n",
    "lr = 0.001 # learning rate\n",
    "\n",
    "# V1. Gradient descent\n",
    "opt = Descent(lr)\n",
    "\n",
    "# V2. ADAM\n",
    "#decay = 0.9\n",
    "#momentum =0.999\n",
    "#opt = ADAM(lr, (decay, momentum))\n",
    "\n",
    "epochs = 1000 # Define the number of epochs\n",
    "trainingLosses = zeros(epochs);# Initialize a vector to keep track of the training progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53f61c-ed69-4037-b624-17b20b87fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = Flux.params(model) #initialize weigths\n",
    "p = Progress(epochs; desc = \"Training in progress\"); # Creates a progress bar\n",
    "showProgress = true\n",
    "\n",
    "# Training loop\n",
    "@time for ii in 1:epochs\n",
    "\n",
    "    Flux.train!(loss, ps, trainingData, opt)\n",
    "\n",
    "    # Update progress indicator\n",
    "    if showProgress\n",
    "        trainingLosses[ii] = mean([loss(x,y) for (x,y) in trainingData])\n",
    "        next!(p; showvalues = [(:loss, trainingLosses[ii]), (:logloss, log10.(trainingLosses[ii]))], valuecolor = :grey)\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f1a02-2cce-4968-a292-5538b7d48549",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_points = 100\n",
    "grid_x = collect(range(lower_bound[1], upper_bound[1], length= nb_points))\n",
    "grid_y = collect(range(lower_bound[2], upper_bound[2], length= nb_points))\n",
    "\n",
    "grid_x_random = [xy[1] for xy in xys]\n",
    "grid_y_random = [xy[2] for xy in xys]\n",
    "\n",
    "Zs = [model([x,y])[1] for (x, y) in zip(grid_x_random, grid_y_random)];\n",
    "\n",
    "# Plot output for trained neural network\n",
    "p1 = plot(grid_x, grid_y, (x, y) -> f([x,y]), label = \"f(x)\", st=:surface)\n",
    "scatter!(p1, grid_x_random, grid_y_random, Zs, label=\"ANN\")\n",
    "title!(\"Original function\")\n",
    "xlabel!(L\"x\")\n",
    "ylabel!(L\"y\")\n",
    "\n",
    "\n",
    "# Plot training loss\n",
    "p2 = plot(1:epochs, log.(trainingLosses), label = \"\", linewidth = 2)\n",
    "title!(\"Training Loss\")\n",
    "xlabel!(\"Epoch\")\n",
    "ylabel!(L\"\\log(\\textrm{Loss})\")\n",
    "\n",
    "# Neural network\n",
    "p3 = plot(grid_x, grid_y, (x, y) -> model([x,y])[1], label = \"f(x)\", st=:surface)\n",
    "title!(\"Trained Neural Network\")\n",
    "xlabel!(L\"x\")\n",
    "ylabel!(L\"y\")\n",
    "\n",
    "ratio = 9/16\n",
    "width = 800\n",
    "pp = plot(p1, p2, p3, size = (width, width*ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e5ae2e-5cf0-4225-9a78-89b87671e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ackley(x; e = exp(1), a = 10.0, b = -0.2, c=2.0*π)\n",
    "    #a, b, c = 10.0, -0.2, 2.0*π\n",
    "    len_recip = inv(length(x))\n",
    "    sum_sqrs = zero(eltype(x))\n",
    "    sum_cos = sum_sqrs\n",
    "    for i in x\n",
    "        sum_cos += cos(c*i)\n",
    "        sum_sqrs += i^2\n",
    "    end\n",
    "    return -a * exp(b * sqrt(len_recip*sum_sqrs)) - exp(len_recip*sum_cos) + a + e\n",
    "end\n",
    "\n",
    "n_samples = 1000\n",
    "lower_bound = [-2.0, -2.0]\n",
    "upper_bound = [2.0, 2.0]\n",
    "xys = Surrogates.sample(n_samples, lower_bound, upper_bound, SobolSample())\n",
    "rawInputs = xys\n",
    "\n",
    "rawOutputs = [[ackley(xy)] for xy in xys] # Compute outputs for each input\n",
    "trainingData = zip(rawInputs, rawOutputs);\n",
    "\n",
    "# Define the neural network layers (this defines a function called model(x))\n",
    "# Specify our model\n",
    "Q1 = 784;\n",
    "Q2 = 50;\n",
    "Q3 = 10;\n",
    "\n",
    "# Two inputs, one output\n",
    "model = Chain(Dense(2,Q1,relu),\n",
    "            Dense(Q1,Q2,relu),\n",
    "            Dense(Q2,1,identity));\n",
    "\n",
    "# Define loss function and weights\n",
    "loss(x, y) = Flux.Losses.mse(model(collect(x)), y)\n",
    "ps = Flux.params(model)\n",
    "\n",
    "# Train the neural network\n",
    "epochs = 1000\n",
    "showProgress = true\n",
    "lr = 0.001 # learning rate\n",
    "\n",
    "# V1. Gradient descent\n",
    "opt = Descent(lr)\n",
    "\n",
    "# V2. ADAM\n",
    "#decay = 0.9\n",
    "#momentum =0.999\n",
    "#opt = ADAM(lr, (decay, momentum))\n",
    "\n",
    "trainingLosses = zeros(epochs) # Initialize vectors to keep track of training\n",
    "p = Progress(epochs; desc = \"Training in progress\") # Creates a progress bar\n",
    "\n",
    "@time for ii in 1:epochs\n",
    "\n",
    "    Flux.train!(loss, ps, trainingData, opt)\n",
    "\n",
    "    # Update progress indicator\n",
    "    if showProgress\n",
    "        trainingLosses[ii] = mean([loss(x,y) for (x,y) in trainingData])\n",
    "        next!(p; showvalues = [(:loss, trainingLosses[ii]), (:logloss, log10.(trainingLosses[ii]))], valuecolor = :grey)\n",
    "    end\n",
    "\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b654af1-8015-4507-b3b4-0196246850ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_points = 1000\n",
    "grid_x = collect(range(lower_bound[1], upper_bound[1], length= nb_points))\n",
    "grid_y = collect(range(lower_bound[2], upper_bound[2], length= nb_points))\n",
    "\n",
    "grid_x_random = [xy[1] for xy in xys]\n",
    "grid_y_random = [xy[2] for xy in xys]\n",
    "\n",
    "Zs = [model([x,y])[1] for (x, y) in zip(grid_x_random, grid_y_random)];\n",
    "\n",
    "# Plot output for trained neural network\n",
    "p1 = plot(grid_x, grid_y, (x, y) -> ackley([x,y]), label = \"f(x)\", st=:surface)\n",
    "# Show somes points, not all of them (otherwise hard to see anything)\n",
    "scatter!(p1, grid_x_random[1:100], grid_y_random[1:100], Zs[1:100], label=\"ANN\")\n",
    "title!(\"Original Function\")\n",
    "xlabel!(L\"x\")\n",
    "ylabel!(L\"y\")\n",
    "\n",
    "\n",
    "# Plot training loss\n",
    "p2 = plot(1:epochs, log.(trainingLosses), label = \"\", linewidth = 2)\n",
    "title!(\"Training Loss\")\n",
    "xlabel!(\"Epoch\")\n",
    "ylabel!(L\"\\log(\\textrm{Loss})\")\n",
    "\n",
    "\n",
    "# Plot output for trained neural network\n",
    "p3 = plot(grid_x, grid_y, (x, y) -> model([x,y])[1], label = \"f(x)\", st=:surface)\n",
    "title!(\"Trained Neural Network\")\n",
    "xlabel!(L\"x\")\n",
    "ylabel!(L\"y\")\n",
    "\n",
    "# Plot difference\n",
    "p4 = plot(grid_x, grid_y, (x, y) -> model([x,y])[1] - ackley([x,y]), label = \"f(x)\", st=:surface)\n",
    "title!(\"Trained Neural Network\")\n",
    "xlabel!(L\"x\")\n",
    "ylabel!(L\"y\")\n",
    "\n",
    "ratio = 9/16\n",
    "width = 800\n",
    "pp = plot(p1, p2, p3, p4, size = (width, width*ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572f71e-81c3-48ec-aa2c-66a3d5c8f2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.1",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
